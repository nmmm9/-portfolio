# RAG ì±—ë´‡ í•µì‹¬ ë¡œì§
# ì›ë³¸: SKN12-4th-2TEAM/app/services/rag_chatbot.py

from openai import OpenAI
from dotenv import load_dotenv
import os
import chromadb
from chromadb.utils import embedding_functions
import json
from pathlib import Path
from typing import Optional, Dict, List
from sentence_transformers import CrossEncoder
import numpy as np
import torch
from torch.nn.functional import sigmoid


# í˜„ì¬ íŒŒì¼ ê¸°ì¤€ ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
DATA_DIR = BASE_DIR / "data"

FEW_SHOT_PATH = DATA_DIR / "few_shot_examples.json"
CHROMA_DIR = DATA_DIR / "chromadb"

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI API í‚¤ ì„¤ì •
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ ìƒì„±í•˜ê³  OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

client = OpenAI(api_key=api_key)

# Cross-encoder ëª¨ë¸ ì´ˆê¸°í™”
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# ì§ˆë¬¸ ìœ í˜•ë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
QUESTION_TYPE_PROMPTS = {
    "definition": """
    ì´ ì§ˆë¬¸ì€ ESG ê°œë…ì´ë‚˜ ìš©ì–´ì— ëŒ€í•œ **ì •ì˜ ë° ì„¤ëª…**ì„ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.

    ë‹¤ìŒ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”:

    ### 1. ê°œìš” ë° ì •ì˜
    - í•´ë‹¹ ê°œë…ì˜ ëª…í™•í•œ ì •ì˜
    - í•µì‹¬ êµ¬ì„± ìš”ì†Œ ì„¤ëª…

    ### 2. ë°°ê²½ ë° ì¤‘ìš”ì„±
    - ì™œ ì¤‘ìš”í•œì§€
    - ESG ë§¥ë½ì—ì„œì˜ ì˜ë¯¸
    - ê´€ë ¨ ê¸€ë¡œë²Œ ê¸°ì¤€ (GRI, SASB, TCFD ë“±)

    ### 3. ì‹¤ë¬´ ì ìš©
    - ê¸°ì—…ì—ì„œ ì–´ë–»ê²Œ í™œìš©ë˜ëŠ”ì§€
    - ì¸¡ì • ë° ê´€ë¦¬ ë°©ë²•

    **ëª¨ë“  ì„¤ëª…ì—ëŠ” ì œê³µëœ ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©ì„ ì¸ìš©í•˜ì„¸ìš”.**
    ì˜ˆ: "Scope 3ëŠ” ê³µê¸‰ë§ ì „ë°˜ì˜ ê°„ì ‘ ë°°ì¶œì„ ì˜ë¯¸í•©ë‹ˆë‹¤ (ì¶œì²˜: SAMPLE ESGë³´ê³ ì„œ, p.10-15)"
    """,

    "how_to": """
    ì´ ì§ˆë¬¸ì€ ESG ì‹¤í–‰ ë°©ë²•ì´ë‚˜ **ì „ëµ ìˆ˜ë¦½**ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.

    ë‹¤ìŒ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”:

    ### 1. ê°œìš”
    - ë¬´ì—‡ì„ ë‹¬ì„±í•˜ë ¤ëŠ” ê²ƒì¸ì§€ ëª…í™•íˆ ì„¤ëª…

    ### 2. í•µì‹¬ ì§€í‘œ (KPI)
    - ì¸¡ì •í•´ì•¼ í•  ì£¼ìš” ì§€í‘œ
    - ëª©í‘œ ìˆ˜ì¹˜ ë° ê¸°ì¤€
    - ì˜ˆ: "ì¬ìƒì—ë„ˆì§€ ì‚¬ìš©ë¥ : 2023ë…„ 15% â†’ 2030ë…„ 50% (ì¶œì²˜: SAMPLE, p.15-18)"

    ### 3. ë‹¨ê³„ë³„ ì‹¤í–‰ ë¡œë“œë§µ
    **ë‹¨ê³„ë³„ë¡œ êµ¬ì²´ì ì¸ ì•¡ì…˜ í”Œëœì„ ì œì‹œí•˜ì„¸ìš”:**

    #### 1ë‹¨ê³„ (0~6ê°œì›”): ì¤€ë¹„ ë° ì§„ë‹¨
    - êµ¬ì²´ì  ì‹¤í–‰ í•­ëª©
    - í•„ìš” ë¦¬ì†ŒìŠ¤

    #### 2ë‹¨ê³„ (7~12ê°œì›”): ì´ˆê¸° ì‹¤í–‰
    - êµ¬ì²´ì  ì‹¤í–‰ í•­ëª©
    - ì˜ˆìƒ ì„±ê³¼

    #### 3ë‹¨ê³„ (1~2ë…„): í™•ëŒ€ ë° ì •ì°©
    - êµ¬ì²´ì  ì‹¤í–‰ í•­ëª©
    - ëª©í‘œ ë‹¬ì„± ê¸°ì¤€

    ### 4. ì°¸ê³  ì‚¬ë¡€
    - ë¬¸ì„œì—ì„œ ì–¸ê¸‰ëœ êµ¬ì²´ì  ì‚¬ë¡€
    - ì„±ê³¼ ë°ì´í„° í¬í•¨

    **ëª¨ë“  ë‚´ìš©ì— ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.**
    """,

    "case_study": """
    ì´ ì§ˆë¬¸ì€ **íŠ¹ì • ê¸°ì—…ì˜ ESG ì‚¬ë¡€**ë¥¼ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.

    ë‹¤ìŒ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”:

    ### 1. ê¸°ì—… ê°œìš”
    - í•´ë‹¹ ê¸°ì—…ì˜ ESG ì „ëµ ë°©í–¥

    ### 2. ì£¼ìš” í™œë™ ë° ì„±ê³¼
    **í‘œ í˜•ì‹ìœ¼ë¡œ ì •ë¦¬í•˜ì„¸ìš”:**

    | ë¶„ì•¼ | ì£¼ìš” í™œë™ | êµ¬ì²´ì  ì„±ê³¼ | ì¶œì²˜ |
    |------|-----------|-------------|------|
    | í™˜ê²½ | ... | ... | (ì¶œì²˜: ..., p.X) |
    | ì‚¬íšŒ | ... | ... | (ì¶œì²˜: ..., p.X) |

    ### 3. íŠ¹ì§• ë° ì‹œì‚¬ì 
    - í•´ë‹¹ ê¸°ì—… í™œë™ì˜ íŠ¹ì§•
    - ë‹¤ë¥¸ ê¸°ì—…ì´ ì°¸ê³ í•  ë§Œí•œ ì 

    ### 4. ì„±ê³¼ ë°ì´í„°
    - êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì§€í‘œ
    - ì „ë…„ ëŒ€ë¹„ ê°œì„ ë„

    **ë°˜ë“œì‹œ ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œë§Œ ì‘ì„±í•˜ê³ , ëª¨ë“  ì£¼ì¥ì— ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.**
    """,

    "comparison": """
    ì´ ì§ˆë¬¸ì€ **ë¹„êµ ë¶„ì„**ì„ ìš”ì²­í•˜ëŠ” ì§ˆë¬¸ì…ë‹ˆë‹¤.

    ë‹¤ìŒ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”:

    ### 1. ë¹„êµ ê°œìš”
    - ë¬´ì—‡ì„ ë¹„êµí•˜ëŠ”ì§€ ëª…í™•íˆ ì„¤ëª…

    ### 2. ë¹„êµ ë¶„ì„
    **í‘œ í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”:**

    | í•­ëª© | A | B | ì°¨ì´ì  |
    |------|---|---|--------|
    | ì§€í‘œ1 | ... | ... | ... |
    | ì§€í‘œ2 | ... | ... | ... |

    ### 3. ì¢…í•© ë¶„ì„
    - ì£¼ìš” ì°¨ì´ì 
    - ê°ê°ì˜ ì¥ë‹¨ì 
    - ìƒí™©ë³„ ì„ íƒ ê¸°ì¤€

    **ëª¨ë“  ë°ì´í„°ì— ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”.**
    """,

    "trend": """
    ì´ ì§ˆë¬¸ì€ **ESG íŠ¸ë Œë“œë‚˜ ë³€í™”**ì— ëŒ€í•œ ì§ˆë¬¸ì…ë‹ˆë‹¤.

    ë‹¤ìŒ êµ¬ì¡°ë¡œ ë‹µë³€í•˜ì„¸ìš”:

    ### 1. í˜„í™© ë¶„ì„
    - í˜„ì¬ ìƒí™© ì„¤ëª…
    - ê´€ë ¨ ë°ì´í„° ì œì‹œ

    ### 2. ë³€í™” ì¶”ì´
    - ì‹œê°„ì— ë”°ë¥¸ ë³€í™”
    - ì£¼ìš” ì „í™˜ì  ë° ì›ì¸

    ### 3. í–¥í›„ ì „ë§
    - ì˜ˆìƒë˜ëŠ” ë³€í™”
    - ì¤€ë¹„í•´ì•¼ í•  ì‚¬í•­

    ### 4. ëŒ€ì‘ ë°©ì•ˆ
    - ê¸°ì—…ì´ ì·¨í•´ì•¼ í•  ì•¡ì…˜
    - ìš°ì„ ìˆœìœ„ ì œì‹œ

    **ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±í•˜ë˜, ì¶”ì„¸ ë¶„ì„ì€ ë…¼ë¦¬ì ìœ¼ë¡œ ë„ì¶œí•˜ì„¸ìš”.**
    """
}

def classify_question_type(query: str) -> str:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ìœ í˜•ì„ ìë™ ë¶„ë¥˜"""

    classification_prompt = """
    ë‹¤ìŒ ESG ê´€ë ¨ ì§ˆë¬¸ì˜ ìœ í˜•ì„ ë¶„ë¥˜í•˜ì„¸ìš”.

    ê°€ëŠ¥í•œ ìœ í˜•:
    - "definition": ê°œë…, ìš©ì–´ ì„¤ëª… ìš”ì²­ (ì˜ˆ: "Scope 3ê°€ ë­ì•¼?", "ESGë€?", "RE100 ì„¤ëª…í•´ì¤˜")
    - "how_to": ì‹¤í–‰ ë°©ë²•, ì „ëµ ìˆ˜ë¦½ (ì˜ˆ: "íƒ„ì†Œë°°ì¶œ ì–´ë–»ê²Œ ì¤„ì—¬?", "ì¬ìƒì—ë„ˆì§€ ëª©í‘œ ì„¤ì •ë²•", "ESG ê²½ì˜ ë„ì… ë°©ë²•")
    - "case_study": íŠ¹ì • ê¸°ì—… ì‚¬ë¡€ (ì˜ˆ: "CJëŠ” ì–´ë–»ê²Œ í•´?", "ì‹ í•œì˜ ESG í™œë™", "ì‚¼í‘œ ì‚¬ë¡€")
    - "comparison": ë¹„êµ ë¶„ì„ (ì˜ˆ: "Aì™€ B ë¹„êµ", "ì°¨ì´ì ì€?", "ì–´ë–¤ ê²Œ ë‚˜ì•„?")
    - "trend": íŠ¸ë Œë“œ, ë³€í™” (ì˜ˆ: "ìµœê·¼ íŠ¸ë Œë“œ", "ESG ë³€í™”", "ì•ìœ¼ë¡œ ì–´ë–»ê²Œ ë ê¹Œ?")
    - "data_inquiry": ë°ì´í„° ì¡°íšŒ (ì˜ˆ: "íƒ„ì†Œë°°ì¶œëŸ‰ì€?", "ëª©í‘œëŠ”?", "ì‹¤ì  ì•Œë ¤ì¤˜")

    ì§ˆë¬¸: {query}

    ë‹¨ìˆœíˆ ìœ í˜•ë§Œ ë°˜í™˜í•˜ì„¸ìš”. ì˜ˆ: definition
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # ë¶„ë¥˜ëŠ” mini ëª¨ë¸ë¡œ ì¶©ë¶„
            messages=[
                {"role": "user", "content": classification_prompt.format(query=query)}
            ],
            temperature=0.1,
            max_tokens=20
        )
        question_type = response.choices[0].message.content.strip().lower()

        # ìœ íš¨í•œ íƒ€ì…ì¸ì§€ í™•ì¸
        valid_types = ["definition", "how_to", "case_study", "comparison", "trend", "data_inquiry"]
        if question_type not in valid_types:
            question_type = "data_inquiry"  # ê¸°ë³¸ê°’

        print(f"ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜: {question_type}")
        return question_type

    except Exception as e:
        print(f"ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜ ì¤‘ ì˜¤ë¥˜: {e}")
        return "data_inquiry"  # ê¸°ë³¸ê°’

def expand_query(query: str, min_length: int = 10) -> str:
    """ì§§ì€ ì¿¼ë¦¬ë¥¼ LLMì„ ì‚¬ìš©í•˜ì—¬ í™•ì¥"""
    if len(query) > min_length: # ì¿¼ë¦¬ê°€ ìµœì†Œ ê¸¸ì´ë³´ë‹¤ í¬ë©´ ì¿¼ë¦¬ ë°˜í™˜
        return query

    system_prompt = """ë„ˆëŠ” ì‚¬ìš©ìì˜ ì§§ì€ ì§ˆë¬¸ì„ ESG ì»¨í…ìŠ¤íŠ¸ì— ë§ê²Œ ë” êµ¬ì²´ì ì´ê³  í’ë¶€í•˜ê²Œ ë°”ê¿”ì£¼ëŠ” ì „ë¬¸ê°€ì•¼.
    ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì•¼ í•´:
    1. ì›ë˜ ì§ˆë¬¸ì˜ ì˜ë„ë¥¼ ìœ ì§€í•˜ë©´ì„œ í™•ì¥
    2. ESG ê´€ë ¨ êµ¬ì²´ì ì¸ ìš©ì–´ë‚˜ ê°œë… í¬í•¨
    3. ê²€ìƒ‰ì— ë„ì›€ë  ë§Œí•œ ê´€ë ¨ í‚¤ì›Œë“œ ì¶”ê°€
    4. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±
    5. í™•ì¥ëœ ì§ˆë¬¸ì€ 1-2ë¬¸ì¥ìœ¼ë¡œ ì œí•œ

    ì˜ˆì‹œ:
    ì…ë ¥: "íƒ„ì†Œë°°ì¶œëŸ‰?"
    ì¶œë ¥: "ê¸°ì—…ì˜ íƒ„ì†Œë°°ì¶œëŸ‰ ê´€ë¦¬ì™€ ê°ì¶• ëª©í‘œëŠ” ì–´ë–»ê²Œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©°, ì–´ë–¤ ê°ì¶• ì „ëµì„ ì‚¬ìš©í•˜ê³  ìˆë‚˜ìš”?"

    ì…ë ¥: "ì§€ë°°êµ¬ì¡°"
    ì¶œë ¥: "ê¸°ì—…ì˜ ì´ì‚¬íšŒ êµ¬ì„±ê³¼ ìš´ì˜ì²´ê³„ëŠ” ì–´ë–»ê²Œ ë˜ì–´ìˆìœ¼ë©°, ì§€ë°°êµ¬ì¡°ì˜ íˆ¬ëª…ì„±ì„ ì–´ë–»ê²Œ í™•ë³´í•˜ê³  ìˆë‚˜ìš”?"
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"ë‹¤ìŒ ì§ˆë¬¸ì„ í™•ì¥í•´ì£¼ì„¸ìš”: {query}"}
            ],
            temperature=0.3,
            max_tokens=200
        )
        expanded_query = response.choices[0].message.content.strip()
        print(f"\nì›ë˜ ì§ˆë¬¸: {query}")
        print(f"í™•ì¥ëœ ì§ˆë¬¸: {expanded_query}\n")
        return expanded_query
    except Exception as e:
        print(f"ì§ˆë¬¸ í™•ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return query

def extract_metadata_filters(query: str) -> Dict[str, str]:
    """ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ"""
    filters = {}

    # ì„¹ì…˜ í•„í„° ì¶”ì¶œ
    sections = {
        "Environment": ["í™˜ê²½", "environment", "ê¸°í›„", "íƒ„ì†Œ"],
        "Social": ["ì‚¬íšŒ", "social", "ì§ì›", "ì¸ê¶Œ", "ì•ˆì „"],
        "Governance": ["ì§€ë°°êµ¬ì¡°", "governance", "ìœ¤ë¦¬", "ì¤€ë²•"]
    }

    query_lower = query.lower()
    for section, keywords in sections.items():
        if any(keyword in query_lower for keyword in keywords):
            filters["section"] = section
            break

    # íšŒì‚¬ëª…ìœ¼ë¡œ í•„í„°ë§
    companies = {
        "CJ": ["cj", "ì”¨ì œì´"],
        "HYUNDAI": ["hyundai", "í˜„ëŒ€", "hdai"],
        "KB": ["kb", "ì¼€ì´ë¹„", "kbê¸ˆìœµ"],
        "LG CHEM": ["lg chem", "lgí™”í•™", "ì—˜ì§€í™”í•™", "ì—˜ì§€ì¼"],
        "LG ELECTRONICS": ["lg electronics", "lgì „ì", "ì—˜ì§€ì „ì", "ì—˜ì§€"],
        "POSCO": ["posco", "í¬ìŠ¤ì½”"],
        "SAMSUNG": ["samsung", "ì‚¼ì„±"],
        "SK": ["sk", "ì—ìŠ¤ì¼€ì´"],
        # ë ˆê±°ì‹œ (ìƒ˜í”Œ ë°ì´í„°)
        "KTNG": ["ktng", "ì¼€ì´í‹°ì•¤ì§€", "kt&g"],
        "SHINHAN": ["ì‹ í•œ", "shinhan"],
        "SAMPYO": ["ì‚¼í‘œ", "sampyo"],
        "SAMPLE": ["sample", "ìƒ˜í”Œ"]
    }

    for company, keywords in companies.items():
        if any(keyword in query_lower for keyword in keywords):
            filters["source"] = company
            break

    return filters

def get_relevance_label(score: float) -> str:
    """ê´€ë ¨ë„ ì ìˆ˜ì— ë”°ë¥¸ ë ˆì´ë¸” ë°˜í™˜"""
    if score > 0.9:
        return "ë§¤ìš° ë†’ì€ ê´€ë ¨ì„± ğŸŒŸ"
    elif score > 0.7:
        return "ë†’ì€ ê´€ë ¨ì„± â­"
    elif score > 0.5:
        return "ì¤‘ê°„ ê´€ë ¨ì„± âœ¨"
    else:
        return "ë‚®ì€ ê´€ë ¨ì„± âšª"

def rerank_documents(query: str, documents: List[str], metadata_list: List[Dict], top_k: int = 5) -> tuple:
    """Cross-encoderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¬ìˆœìœ„í™” (ì •ê·œí™” í¬í•¨)"""
    # ê° ë¬¸ì„œì™€ ì¿¼ë¦¬ì˜ ìŒì„ ìƒì„±
    pairs = [[query, doc] for doc in documents]

    # Cross-encoderë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚° (Tensorë¡œ ë°˜í™˜)
    raw_scores = cross_encoder.predict(pairs, convert_to_numpy=False)

    # sigmoidë¡œ ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„)
    if not isinstance(raw_scores, torch.Tensor):
        raw_scores = torch.tensor(raw_scores)
    norm_scores = sigmoid(raw_scores).tolist()

    # ì ìˆ˜ì— ë”°ë¼ ë¬¸ì„œ ì •ë ¬
    doc_score_pairs = list(zip(documents, metadata_list, norm_scores))
    doc_score_pairs.sort(key=lambda x: x[2], reverse=True)

    # top_k ê°œì˜ ë¬¸ì„œ ì„ íƒ
    reranked_docs = []
    reranked_metadata = []
    reranked_scores = []

    for doc, metadata, score in doc_score_pairs[:top_k]:
        reranked_docs.append(doc)
        reranked_metadata.append(metadata)
        reranked_scores.append(score)

    return reranked_docs, reranked_metadata, reranked_scores

def get_relevant_context(query: str, collection, initial_k: int = 20, final_k: int = 5, metadata_filters: Optional[Dict[str, str]] = None) -> tuple:
    """ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œ ê²€ìƒ‰ (2ë‹¨ê³„ ê²€ìƒ‰)"""
    try:
        # metadata_filtersë¥¼ ChromaDB where ì ˆ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        where_clause = None
        if metadata_filters and len(metadata_filters) > 0:
            conditions = []
            for field, value in metadata_filters.items():
                conditions.append({
                    field: {"$eq": value}
                })

            if len(conditions) == 1:
                where_clause = conditions[0]
            else:
                where_clause = {
                    "$and": conditions
                }

            print(f"ì ìš©ëœ ê²€ìƒ‰ í•„í„°: {where_clause}")

        # 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ initial_kê°œ ë¬¸ì„œ ê²€ìƒ‰
        results = collection.query(
            query_texts=[query],
            n_results=initial_k,
            where=where_clause
        )

        # ê²°ê³¼ê°€ ì—†ìœ¼ë©´ í•„í„° ì—†ì´ ë‹¤ì‹œ ê²€ìƒ‰
        if not results['documents'][0]:
            print("ì§€ì •ëœ í•„í„°ë¡œ ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì—†ì–´ ì „ì²´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
            results = collection.query(
                query_texts=[query],
                n_results=initial_k
            )
    except Exception as e:
        print(f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("í•„í„° ì—†ì´ ì „ì²´ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
        results = collection.query(
            query_texts=[query],
            n_results=initial_k
        )

    # 2ë‹¨ê³„: Cross-encoderë¡œ ì¬ìˆœìœ„í™”
    print(f"ë””ë²„ê¹…: ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜ = {len(results['documents'][0]) if results['documents'] and results['documents'][0] else 0}")

    if results['documents'] and results['documents'][0]:
        reranked_docs, reranked_metadata, scores = rerank_documents(
            query,
            results['documents'][0],
            results['metadatas'][0],
            final_k
        )
    else:
        print("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return "", {
            "sections": set(),
            "subsections": set(),
            "sources": set(),
            "page_ranges": set()
        }

    # ë©”íƒ€ë°ì´í„° ìš”ì•½ ì •ë³´ ìˆ˜ì§‘
    metadata_summary = {
        "sections": set(),
        "subsections": set(),
        "sources": set(),
        "page_ranges": set()
    }

    contexts = []
    for i, (doc, metadata, score) in enumerate(zip(reranked_docs, reranked_metadata, scores)):
        metadata_summary["sections"].add(metadata['section'])
        metadata_summary["subsections"].add(metadata['sub_section'])
        metadata_summary["sources"].add(metadata['source'])
        metadata_summary["page_ranges"].add(metadata.get('page_range', 'ì•Œ ìˆ˜ ì—†ìŒ'))

        relevance_label = get_relevance_label(score)

        context = f"""
                    ì¶œì²˜: {metadata['source']}
                    ì„¹ì…˜: {metadata['section']}
                    ì„œë¸Œì„¹ì…˜: {metadata['sub_section']}
                    í˜ì´ì§€: {metadata.get('page_range', 'ì•Œ ìˆ˜ ì—†ìŒ')}
                    ê´€ë ¨ë„: {score:.4f} ({relevance_label})
                    ë‚´ìš©: {doc}
                    ---"""
        contexts.append(context)

    return "\n".join(contexts), metadata_summary

def generate_response(query: str, context: str, metadata_summary: Dict, question_type: str = "data_inquiry"):
    """ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±"""
    # few-shot ì˜ˆì‹œ ë¡œë“œ
    examples_path = FEW_SHOT_PATH
    with open(examples_path, "r", encoding="utf-8") as f:
        few_shot_examples = json.load(f)

    # ë©”íƒ€ë°ì´í„° ìš”ì•½ ë¬¸ìì—´ ìƒì„±
    metadata_info = f"""
    ì°¸ê³ í•œ ë¬¸ì„œ ì •ë³´:
    - ì„¹ì…˜: {', '.join(sorted(metadata_summary['sections']))}
    - ì„œë¸Œì„¹ì…˜: {', '.join(sorted(metadata_summary['subsections']))}
    - ì¶œì²˜: {', '.join(sorted(metadata_summary['sources']))}
    - í˜ì´ì§€: {', '.join(sorted(str(p) for p in metadata_summary['page_ranges']))}
    """

    # ì§ˆë¬¸ ìœ í˜•ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„ íƒ
    type_specific_prompt = QUESTION_TYPE_PROMPTS.get(question_type, "")

    # ê°œì„ ëœ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
    system_prompt = f"""ë‹¹ì‹ ì€ ê¸°ì—…ì˜ ESG(í™˜ê²½Â·ì‚¬íšŒÂ·ì§€ë°°êµ¬ì¡°) ê²½ì˜ì„ ì§€ì›í•˜ëŠ” ì „ë¬¸ AI ì±—ë´‡ì…ë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™:**
1. **ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€**: ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ë¬¸ì„œì— ì—†ëŠ” ì •ë³´ëŠ” ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.
2. **ì¶œì²˜ ëª…ì‹œ**: ëª¨ë“  ì£¼ìš” ì •ë³´ì—ëŠ” ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”. í˜•ì‹: (ì¶œì²˜: [íšŒì‚¬ëª…] ESGë³´ê³ ì„œ, p.[í˜ì´ì§€])
3. **ì •í™•ì„±**: ìˆ˜ì¹˜, ë‚ ì§œ, ê³ ìœ ëª…ì‚¬ëŠ” ë¬¸ì„œ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”.
4. **êµ¬ì¡°í™”**: Markdown í˜•ì‹ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

---

{type_specific_prompt}

---

**ë‹µë³€ ì‹œ ì£¼ì˜ì‚¬í•­:**
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” "ë¬¸ì„œì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ëª…ì‹œ
- ì—¬ëŸ¬ ì¶œì²˜ì˜ ì •ë³´ë¥¼ ì¢…í•©í•  ë•ŒëŠ” ê°ê° ì¶œì²˜ í‘œì‹œ
- í‘œë‚˜ ë¦¬ìŠ¤íŠ¸ë¥¼ í™œìš©í•˜ì—¬ ê°€ë…ì„± í–¥ìƒ
- ë§ˆì§€ë§‰ì— í•µì‹¬ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½

---

**ì œê³µëœ ë¬¸ì„œ ì •ë³´:**
{metadata_info}

**ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:**
{context}

---

ìœ„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”."""

    try:
        result = client.chat.completions.create(
            model="gpt-4o",  # GPT-4o ëª¨ë¸ ì‚¬ìš©
            messages = [{"role": "system", "content": system_prompt}] +
                        few_shot_examples +
                        [{"role": "user", "content": query}],
            temperature=0.7,
            max_tokens=1500  # ë” ê¸´ ë‹µë³€ í—ˆìš©
        )
        return result.choices[0].message.content, metadata_info
    except Exception as e:
        print(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì‘ë‹µì„ ìƒì„±í•˜ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", metadata_info

def verify_answer(query: str, answer: str, context: str) -> dict:
    """ë‹µë³€ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ì‹ ë¢°ë„ ì ìˆ˜ ë°˜í™˜"""

    verification_prompt = f"""
    ë‹¤ìŒ ESG ì§ˆë¬¸ê³¼ ë‹µë³€ì„ í‰ê°€í•˜ì„¸ìš”.

    **ì§ˆë¬¸:** {query}

    **ë‹µë³€:** {answer}

    **ì›ë³¸ ë¬¸ì„œ:** {context}

    ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ í‰ê°€í•˜ì„¸ìš”:
    1. **ê´€ë ¨ì„±** (0-10): ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ì–¼ë§ˆë‚˜ ê´€ë ¨ìˆëŠ”ê°€?
    2. **ì •í™•ì„±** (0-10): ë‹µë³€ì´ ì›ë³¸ ë¬¸ì„œì™€ ì¼ì¹˜í•˜ëŠ”ê°€? (ìˆ˜ì¹˜, ë‚ ì§œ í¬í•¨)
    3. **ì™„ì „ì„±** (0-10): ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µë³€í–ˆëŠ”ê°€?
    4. **ì¶œì²˜ í‘œì‹œ** (0-10): ì¶œì²˜ê°€ ëª…í™•íˆ í‘œì‹œë˜ì—ˆëŠ”ê°€?

    JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜:
    {{
        "relevance": ì ìˆ˜,
        "accuracy": ì ìˆ˜,
        "completeness": ì ìˆ˜,
        "citation": ì ìˆ˜,
        "overall": í‰ê· ì ìˆ˜,
        "issues": ["ë¬¸ì œì 1", "ë¬¸ì œì 2"] (ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´),
        "confidence": "high/medium/low"
    }}
    """

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # ê²€ì¦ì€ minië¡œ ì¶©ë¶„
            messages=[
                {"role": "user", "content": verification_prompt}
            ],
            temperature=0.1,
            max_tokens=300
        )

        # JSON íŒŒì‹±
        import json
        result_text = response.choices[0].message.content.strip()

        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
        if "```json" in result_text:
            result_text = result_text.split("```json")[1].split("```")[0].strip()
        elif "```" in result_text:
            result_text = result_text.split("```")[1].split("```")[0].strip()

        verification_result = json.loads(result_text)

        print(f"\në‹µë³€ ê²€ì¦ ê²°ê³¼:")
        print(f"  ì‹ ë¢°ë„: {verification_result.get('confidence', 'unknown')}")
        print(f"  ì¢…í•© ì ìˆ˜: {verification_result.get('overall', 0)}/10")

        if verification_result.get('issues'):
            print(f"  ë¬¸ì œì : {', '.join(verification_result['issues'])}")

        return verification_result

    except Exception as e:
        print(f"ë‹µë³€ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
        return {
            "overall": 7.0,
            "confidence": "medium",
            "issues": []
        }

def run_rag_pipeline(user_query: str, collection, enable_verification: bool = True) -> dict:
    """ê°œì„ ëœ RAG íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

    # 1ë‹¨ê³„: ì§ˆë¬¸ ìœ í˜• ë¶„ë¥˜
    question_type = classify_question_type(user_query)

    # 2ë‹¨ê³„: ì¿¼ë¦¬ í™•ì¥
    expanded = expand_query(user_query)

    # 3ë‹¨ê³„: ë©”íƒ€ë°ì´í„° í•„í„° ì¶”ì¶œ
    filters = extract_metadata_filters(user_query)

    # 4-5ë‹¨ê³„: ë¬¸ì„œ ê²€ìƒ‰ + ì¬ìˆœìœ„í™”
    context, metadata = get_relevant_context(expanded, collection, metadata_filters=filters)

    # 6ë‹¨ê³„: ë‹µë³€ ìƒì„± (ì§ˆë¬¸ ìœ í˜• ë°˜ì˜)
    answer, relevance = generate_response(user_query, context, metadata, question_type)

    # 7ë‹¨ê³„: ë‹µë³€ ê²€ì¦ (ì„ íƒì )
    verification = None
    if enable_verification and context:
        verification = verify_answer(user_query, answer, context)

    return {
        "answer": answer,
        "relevance": relevance,
        "context": context,
        "question_type": question_type,
        "verification": verification
    }
