# Vision RAG ì±—ë´‡ (ëŒ€í™” ë¬¸ë§¥ ìœ ì§€)
# ESG ë³´ê³ ì„œ ê¸°ë°˜ AI ì±—ë´‡

from openai import OpenAI
from dotenv import load_dotenv
import os
import chromadb
from chromadb.utils import embedding_functions
from pathlib import Path
from typing import List, Dict
from sentence_transformers import CrossEncoder
import torch
from torch.nn.functional import sigmoid

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

client = OpenAI(api_key=api_key)

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
CHROMA_DIR = BASE_DIR / "data" / "chromadb_vision"

# Cross-encoder ëª¨ë¸ ì´ˆê¸°í™”
cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
SYSTEM_PROMPT = """ë‹¹ì‹ ì€ **ESG(í™˜ê²½Â·ì‚¬íšŒÂ·ì§€ë°°êµ¬ì¡°) ì „ë¬¸ AI ì»¨ì„¤í„´íŠ¸**ì…ë‹ˆë‹¤.
ê¸°ì—…ì˜ ESG ë³´ê³ ì„œë¥¼ ë¶„ì„í•˜ì—¬ ì •í™•í•˜ê³  í†µì°°ë ¥ ìˆëŠ” ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ¯ í•µì‹¬ ì›ì¹™

### 1. ì •í™•ì„± (Accuracy)
- **ë°˜ë“œì‹œ** ì œê³µëœ ë¬¸ì„œ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
- ìˆ˜ì¹˜, ë‚ ì§œ, ê³ ìœ ëª…ì‚¬ëŠ” ë¬¸ì„œ ê·¸ëŒ€ë¡œ ì •í™•íˆ ì¸ìš©í•˜ì„¸ìš”
- ì¶”ì¸¡ì´ë‚˜ ì¼ë°˜ì ì¸ ì§€ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”

### 2. ì¶œì²˜ ëª…ì‹œ (Citation)
- ëª¨ë“  ì£¼ìš” ì •ë³´ì— ì¶œì²˜ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
- í˜•ì‹: **(ì¶œì²˜: [íšŒì‚¬ëª…] [ë…„ë„] ESGë³´ê³ ì„œ, p.[í˜ì´ì§€])**
- ì—¬ëŸ¬ ì¶œì²˜ë¥¼ ì¢…í•©í•  ë•ŒëŠ” ê°ê° í‘œì‹œí•˜ì„¸ìš”

### 3. êµ¬ì¡°í™”ëœ ë‹µë³€ (Structure)
- Markdown í˜•ì‹ìœ¼ë¡œ ì²´ê³„ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
- ë³µì¡í•œ ì •ë³´ëŠ” í‘œ, ëª©ë¡, ì†Œì œëª©ì„ í™œìš©í•˜ì„¸ìš”
- í•µì‹¬ ë‚´ìš©ì„ ë¨¼ì € ì œì‹œí•˜ê³  ì„¸ë¶€ì‚¬í•­ì„ ì„¤ëª…í•˜ì„¸ìš”

### 4. ëŒ€í™” ë¬¸ë§¥ (Context)
- ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì¼ê´€ì„± ìˆê²Œ ë‹µë³€í•˜ì„¸ìš”
- "ê·¸ê²ƒ", "ì´ì „ì— ë§í•œ" ë“± ëŒ€ëª…ì‚¬ê°€ ê°€ë¦¬í‚¤ëŠ” ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”
- ê°™ì€ ì •ë³´ë¥¼ ë°˜ë³µí•˜ì§€ ë§ê³  ìƒˆë¡œìš´ ê´€ì ì„ ì¶”ê°€í•˜ì„¸ìš”

## ğŸ“‹ ë‹µë³€ í˜•ì‹

### ì¼ë°˜ ì§ˆë¬¸:
```
**[í•µì‹¬ ë‹µë³€]**

[ìƒì„¸ ì„¤ëª…]

ğŸ“Š **ê´€ë ¨ ìˆ˜ì¹˜:**
- í•­ëª©1: ìˆ˜ì¹˜ (ì¶œì²˜)
- í•­ëª©2: ìˆ˜ì¹˜ (ì¶œì²˜)

ğŸ’¡ **ì‹œì‚¬ì :** [ë¶„ì„ ë˜ëŠ” ì˜ë¯¸]
```

### ë¹„êµ/ë¶„ì„ ì§ˆë¬¸:
```
**[ë¹„êµ ìš”ì•½]**

| í•­ëª© | A | B | ë¹„ê³  |
|------|---|---|------|
| ì§€í‘œ1 | ê°’ | ê°’ | ë¶„ì„ |

ğŸ“ˆ **ë¶„ì„:** [ì°¨ì´ì ê³¼ ì˜ë¯¸]
```

### ì •ë³´ ë¶€ì¡± ì‹œ:
```
ì œê³µëœ ë¬¸ì„œì—ì„œ [ì§ˆë¬¸ ë‚´ìš©]ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.

**ê´€ë ¨ ì •ë³´:**
- [ì°¾ì„ ìˆ˜ ìˆëŠ” ìœ ì‚¬ ì •ë³´]

ğŸ’¡ **ëŒ€ì•ˆ:** [ë‹¤ë¥¸ ì§ˆë¬¸ ì œì•ˆ]
```

## ğŸ” ESG ë¶„ì•¼ë³„ ì „ë¬¸ì„±

### í™˜ê²½ (Environment)
- íƒ„ì†Œë°°ì¶œëŸ‰ (Scope 1, 2, 3), ì˜¨ì‹¤ê°€ìŠ¤, ì—ë„ˆì§€ ì‚¬ìš©
- ì¬ìƒì—ë„ˆì§€, íê¸°ë¬¼, ìˆ˜ìì›, ìƒë¬¼ë‹¤ì–‘ì„±
- ê¸°í›„ë³€í™” ëŒ€ì‘, íƒ„ì†Œì¤‘ë¦½ ëª©í‘œ

### ì‚¬íšŒ (Social)
- ì„ì§ì› (ë‹¤ì–‘ì„±, ì•ˆì „, ë³µì§€, êµìœ¡)
- ê³µê¸‰ë§ ê´€ë¦¬, ì¸ê¶Œ, ì§€ì—­ì‚¬íšŒ ê³µí—Œ
- ê³ ê° ë§Œì¡±, ê°œì¸ì •ë³´ ë³´í˜¸

### ì§€ë°°êµ¬ì¡° (Governance)
- ì´ì‚¬íšŒ êµ¬ì„±, ìœ¤ë¦¬ê²½ì˜, ì¤€ë²•ê²½ì˜
- ë¦¬ìŠ¤í¬ ê´€ë¦¬, ì£¼ì£¼ê¶Œë¦¬, ì •ë³´ê³µê°œ

## âš ï¸ ì£¼ì˜ì‚¬í•­
- í™•ì‹¤í•˜ì§€ ì•Šì€ ì •ë³´ëŠ” "ë¬¸ì„œì—ì„œ í™•ì¸ í•„ìš”"ë¼ê³  ëª…ì‹œ
- ë¯¼ê°í•œ ì •ë³´(ë¯¸ê³µê°œ ì „ëµ, ë…¼ìŸì  ì£¼ì œ)ëŠ” ê°ê´€ì ìœ¼ë¡œë§Œ ì„œìˆ 
- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê³  ì „ë¬¸ì ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”
"""


def initialize_chatbot():
    """ì±—ë´‡ ì´ˆê¸°í™”"""
    print("=" * 60)
    print("ğŸ¤– Vision RAG ì±—ë´‡ ì´ˆê¸°í™” ì¤‘...")
    print("=" * 60)

    # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    chroma_client = chromadb.PersistentClient(path=str(CHROMA_DIR))

    # ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="jhgan/ko-sroberta-multitask"
    )

    # ì»¬ë ‰ì…˜ ê°€ì ¸ì˜¤ê¸°
    collection = chroma_client.get_collection(
        name="esg_vision_collection",
        embedding_function=embedding_function
    )

    # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
    doc_count = collection.count()
    print(f"âœ… ì´ {doc_count}ê°œì˜ ë¬¸ì„œê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("=" * 60)

    return collection


def rerank_documents(query: str, documents: List[str], metadata_list: List[Dict], top_k: int = 5):
    """Cross-encoderë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œ ì¬ìˆœìœ„í™”"""
    # ê° ë¬¸ì„œì™€ ì¿¼ë¦¬ì˜ ìŒì„ ìƒì„±
    pairs = [[query, doc] for doc in documents]

    # Cross-encoderë¡œ ìœ ì‚¬ë„ ì ìˆ˜ ê³„ì‚°
    raw_scores = cross_encoder.predict(pairs, convert_to_numpy=False)

    # sigmoidë¡œ ì ìˆ˜ ì •ê·œí™” (0~1 ë²”ìœ„)
    if not isinstance(raw_scores, torch.Tensor):
        raw_scores = torch.tensor(raw_scores)
    norm_scores = sigmoid(raw_scores).tolist()

    # ì ìˆ˜ì— ë”°ë¼ ë¬¸ì„œ ì •ë ¬
    doc_score_pairs = list(zip(documents, metadata_list, norm_scores))
    doc_score_pairs.sort(key=lambda x: x[2], reverse=True)

    # top_k ê°œì˜ ë¬¸ì„œ ì„ íƒ
    return doc_score_pairs[:top_k]


def search_documents(query, collection, initial_k=15, final_k=5):
    """ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ (2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ + Cross-encoder ë¦¬ë­í‚¹)"""

    # 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ë¬¸ì„œ ê²€ìƒ‰
    results = collection.query(
        query_texts=[query],
        n_results=initial_k
    )

    if not results['documents'][0]:
        return ""

    # 2ë‹¨ê³„: Cross-encoderë¡œ ì¬ìˆœìœ„í™”
    reranked = rerank_documents(
        query,
        results['documents'][0],
        results['metadatas'][0],
        final_k
    )

    # ê²°ê³¼ í¬ë§·íŒ…
    contexts = []
    for doc, metadata, score in reranked:
        company = metadata.get('company', '')
        year = metadata.get('year', '')
        page = metadata.get('page', '')
        version = metadata.get('version', '')

        version_str = f" ({version})" if version else ""

        # ê´€ë ¨ë„ í‘œì‹œ
        if score > 0.8:
            relevance = "ğŸŒŸ ë§¤ìš° ë†’ìŒ"
        elif score > 0.6:
            relevance = "â­ ë†’ìŒ"
        elif score > 0.4:
            relevance = "âœ¨ ì¤‘ê°„"
        else:
            relevance = "â—‹ ë‚®ìŒ"

        context = f"""
[ì¶œì²˜: {company} {year}ë…„{version_str} ESG ë³´ê³ ì„œ, {page}í˜ì´ì§€] [{relevance}]
{doc[:2000]}...
"""
        contexts.append(context)

    return "\n---\n".join(contexts)


def generate_response(query: str, context: str, conversation_history: List[Dict[str, str]]):
    """GPT-4oë¡œ ë‹µë³€ ìƒì„± (ëŒ€í™” ê¸°ë¡ í¬í•¨)"""

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì— í˜„ì¬ ê²€ìƒ‰ëœ ë¬¸ì„œ ì¶”ê°€
    system_with_context = f"""{SYSTEM_PROMPT}

## í˜„ì¬ ì§ˆë¬¸ì— ëŒ€í•œ ê²€ìƒ‰ëœ ë¬¸ì„œ:
{context}
"""

    # ë©”ì‹œì§€ êµ¬ì„±: ì‹œìŠ¤í…œ + ëŒ€í™” ê¸°ë¡ + í˜„ì¬ ì§ˆë¬¸
    messages = [{"role": "system", "content": system_with_context}]

    # ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶”ê°€ (ìµœê·¼ 10ê°œê¹Œì§€)
    recent_history = conversation_history[-10:] if len(conversation_history) > 10 else conversation_history
    messages.extend(recent_history)

    # í˜„ì¬ ì§ˆë¬¸ ì¶”ê°€
    messages.append({"role": "user", "content": query})

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=messages,
        temperature=0.3,
        max_tokens=2000
    )

    return response.choices[0].message.content


def chat(query: str, collection, conversation_history: List[Dict[str, str]]):
    """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"""
    # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    context = search_documents(query, collection)

    # 2. ë‹µë³€ ìƒì„± (ëŒ€í™” ê¸°ë¡ í¬í•¨)
    response = generate_response(query, context, conversation_history)

    return response


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ì´ˆê¸°í™”
    collection = initialize_chatbot()

    # ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”
    conversation_history: List[Dict[str, str]] = []

    print("\nğŸ’¡ ESG ë³´ê³ ì„œ ê¸°ë°˜ AI ì±—ë´‡ì…ë‹ˆë‹¤.")
    print("   ì˜ˆì‹œ: 'CJì˜ íƒ„ì†Œë°°ì¶œëŸ‰ì€?', 'CJì˜ ESG ì „ëµì€?'")
    print("   ëŒ€í™” ì´ˆê¸°í™”: 'clear' ë˜ëŠ” 'reset'")
    print("   ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit'\n")

    # ëŒ€í™” ë£¨í”„
    while True:
        query = input("â“ ì§ˆë¬¸: ").strip()

        # ì¢…ë£Œ ëª…ë ¹ì–´
        if query.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
            print("ì±—ë´‡ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # ëŒ€í™” ì´ˆê¸°í™” ëª…ë ¹ì–´
        if query.lower() in ['clear', 'reset', 'ì´ˆê¸°í™”']:
            conversation_history.clear()
            print("ğŸ”„ ëŒ€í™” ê¸°ë¡ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
            continue

        # ë¹ˆ ì…ë ¥ ì²´í¬
        if not query:
            print("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            continue

        print("\nğŸ” ê²€ìƒ‰ ì¤‘...")
        response = chat(query, collection, conversation_history)

        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        conversation_history.append({"role": "user", "content": query})
        conversation_history.append({"role": "assistant", "content": response})

        print("\n" + "=" * 60)
        print("ğŸ“ ë‹µë³€:")
        print("=" * 60)
        print(response)
        print("=" * 60)
        print(f"ğŸ’¬ ëŒ€í™” ê¸°ë¡: {len(conversation_history)//2}ê°œ ëŒ€í™”\n")


if __name__ == "__main__":
    main()
