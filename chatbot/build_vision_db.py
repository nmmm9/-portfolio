# ì „ì²´ PDF Vision ì²˜ë¦¬ ë° ì €ì¥ ìŠ¤í¬ë¦½íŠ¸
# ëª¨ë“  íšŒì‚¬ì˜ ëª¨ë“  PDFë¥¼ ì²˜ë¦¬í•˜ì—¬ JSON + ChromaDBì— ì €ì¥

from openai import OpenAI
from dotenv import load_dotenv
import os
import base64
import json
import re
from io import BytesIO
from pdf2image import convert_from_path
from pathlib import Path
import chromadb
from chromadb.utils import embedding_functions

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
api_key = os.getenv('OPENAI_API_KEY')
if not api_key:
    raise ValueError("OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

client = OpenAI(api_key=api_key)

# ê²½ë¡œ ì„¤ì •
BASE_DIR = Path(__file__).resolve().parent
COMPANY_DIR = BASE_DIR / "company"
DATA_DIR = BASE_DIR / "data"
EXTRACTED_DIR = DATA_DIR / "extracted"
CHROMA_DIR = DATA_DIR / "chromadb_vision"

# ë””ë ‰í† ë¦¬ ìƒì„±
EXTRACTED_DIR.mkdir(parents=True, exist_ok=True)
CHROMA_DIR.mkdir(parents=True, exist_ok=True)


def extract_year_from_filename(filename):
    """íŒŒì¼ëª…ì—ì„œ ë…„ë„ ì¶”ì¶œ"""
    # 2019-2020 í˜•ì‹ (LG ELECTRONICS) - ë’¤ì˜ ë…„ë„ ì‚¬ìš©
    match_range = re.search(r'20\d{2}-(20\d{2})', filename)
    if match_range:
        return int(match_range.group(1))

    # ì¼ë°˜ í˜•ì‹ (2019, 2020, ...)
    match = re.search(r'(20\d{2})', filename)
    if match:
        return int(match.group(1))
    return None


def extract_version_from_filename(filename):
    """íŒŒì¼ëª…ì—ì„œ ë²„ì „ ì •ë³´ ì¶”ì¶œ (KB ë“±)"""
    if "ì´í•´ê´€ê³„ì" in filename:
        return "ì´í•´ê´€ê³„ì"
    elif "íˆ¬ìì" in filename:
        return "íˆ¬ìì"
    return None


def pdf_page_to_base64(pdf_path, page_num=0):
    """PDFì˜ íŠ¹ì • í˜ì´ì§€ë¥¼ base64 ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
    images = convert_from_path(
        pdf_path,
        first_page=page_num + 1,
        last_page=page_num + 1,
        dpi=150
    )

    if not images:
        raise ValueError(f"í˜ì´ì§€ {page_num}ì„ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    image = images[0]
    buffered = BytesIO()
    image.save(buffered, format="PNG")
    img_base64 = base64.b64encode(buffered.getvalue()).decode()

    return img_base64


def analyze_page_with_vision(img_base64, page_num):
    """GPT-4o-mini Visionìœ¼ë¡œ í˜ì´ì§€ ì „ì²´ ë‚´ìš© ì¶”ì¶œ"""

    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[{
            "role": "user",
            "content": [
                {
                    "type": "text",
                    "text": """ì´ ESG ë³´ê³ ì„œ í˜ì´ì§€ì˜ ëª¨ë“  ë‚´ìš©ì„ ì •í™•í•˜ê²Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

## ì¶”ì¶œ ê·œì¹™

### 1. í…ìŠ¤íŠ¸ (TEXT)
- í˜ì´ì§€ì— ìˆëŠ” ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì¶”ì¶œ
- ì œëª©, ë³¸ë¬¸, ìº¡ì…˜, ì£¼ì„ ëª¨ë‘ í¬í•¨
- ì›ë³¸ ìˆœì„œì™€ êµ¬ì¡° ìœ ì§€

### 2. í‘œ (TABLE)
- ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì •í™•íˆ ë³€í™˜
- ëª¨ë“  í–‰ê³¼ ì—´ì˜ ë°ì´í„°ë¥¼ ë¹ ì§ì—†ì´ ì¶”ì¶œ
- ìˆ«ìëŠ” ë‹¨ìœ„(%, ì›, í†¤ ë“±)ì™€ í•¨ê»˜ ì •í™•íˆ ê¸°ì¬
- í‘œ ì œëª©ì´ ìˆìœ¼ë©´ í‘œ ìœ„ì— **[í‘œ: ì œëª©]** í˜•ì‹ìœ¼ë¡œ í‘œì‹œ

### 3. ê·¸ë˜í”„/ì°¨íŠ¸ (CHART)
- **[ì°¨íŠ¸: ì œëª©]** í˜•ì‹ìœ¼ë¡œ ì‹œì‘
- ì°¨íŠ¸ ìœ í˜• ëª…ì‹œ (ë§‰ëŒ€, ì„ , ì›í˜• ë“±)
- ëª¨ë“  ë°ì´í„° í¬ì¸íŠ¸ì˜ ìˆ˜ì¹˜ë¥¼ ì¶”ì¶œ
- Xì¶•, Yì¶• ë ˆì´ë¸”ê³¼ ë²”ë¡€ í¬í•¨
- ì˜ˆì‹œ:
  - 2021ë…„: 1,234í†¤
  - 2022ë…„: 1,456í†¤
  - 2023ë…„: 1,678í†¤

### 4. ì´ë¯¸ì§€/ì•„ì´ì½˜
- **[ì´ë¯¸ì§€: ì„¤ëª…]** í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì„¤ëª…

## ì¶œë ¥ í˜•ì‹

```
## í˜ì´ì§€ ì œëª©: [ì œëª©]

### í…ìŠ¤íŠ¸ ë‚´ìš©
[ì¶”ì¶œëœ ëª¨ë“  í…ìŠ¤íŠ¸]

### í‘œ
[í‘œ: ì œëª©]
| ì—´1 | ì—´2 | ì—´3 |
|-----|-----|-----|
| ë°ì´í„° | ë°ì´í„° | ë°ì´í„° |

### ì°¨íŠ¸/ê·¸ë˜í”„
[ì°¨íŠ¸: ì œëª©]
- ì°¨íŠ¸ ìœ í˜•: ë§‰ëŒ€ ê·¸ë˜í”„
- ë°ì´í„°:
  - í•­ëª©1: ìˆ˜ì¹˜
  - í•­ëª©2: ìˆ˜ì¹˜

### ê¸°íƒ€
[ì´ë¯¸ì§€ë‚˜ íŠ¹ì´ì‚¬í•­]

---

### ğŸ“Œ í˜ì´ì§€ ìš”ì•½
- **í•µì‹¬ ì£¼ì œ**: [ì´ í˜ì´ì§€ì˜ í•µì‹¬ ì£¼ì œ 1ë¬¸ì¥]
- **ì£¼ìš” ë‚´ìš©**: [3-5ë¬¸ì¥ìœ¼ë¡œ í•µì‹¬ ë‚´ìš© ìš”ì•½]
- **í•µì‹¬ ìˆ˜ì¹˜**: [ê°€ì¥ ì¤‘ìš”í•œ ìˆ˜ì¹˜ 3-5ê°œ ë‚˜ì—´]
- **í‚¤ì›Œë“œ**: [í•µì‹¬ í‚¤ì›Œë“œ 5ê°œ]
```

**ì¤‘ìš”: ìˆ«ìì™€ ìˆ˜ì¹˜ëŠ” ë°˜ë“œì‹œ ì •í™•í•˜ê²Œ ì¶”ì¶œí•˜ì„¸ìš”. ì¶”ì¸¡í•˜ì§€ ë§ˆì„¸ìš”.**
**í•œêµ­ì–´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”.**"""
                },
                {
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_base64}"
                    }
                }
            ]
        }],
        max_tokens=6000
    )

    return response.choices[0].message.content


def get_total_pages(pdf_path):
    """PDF ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸"""
    from pdf2image.pdf2image import pdfinfo_from_path
    info = pdfinfo_from_path(pdf_path)
    return info['Pages']


def process_single_pdf(pdf_path, company_name):
    """ë‹¨ì¼ PDF íŒŒì¼ ì²˜ë¦¬"""
    filename = os.path.basename(pdf_path)
    year = extract_year_from_filename(filename)
    version = extract_version_from_filename(filename)

    print(f"\n{'='*60}")
    print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘: {filename}")
    version_str = f", ë²„ì „: {version}" if version else ""
    print(f"   íšŒì‚¬: {company_name}, ë…„ë„: {year}{version_str}")
    print(f"{'='*60}")

    # ì´ í˜ì´ì§€ ìˆ˜ í™•ì¸
    total_pages = get_total_pages(pdf_path)
    print(f"   ì´ í˜ì´ì§€: {total_pages}")

    pages_data = []

    for page_num in range(total_pages):
        try:
            print(f"   í˜ì´ì§€ {page_num + 1}/{total_pages} ì²˜ë¦¬ ì¤‘...", end=" ")

            # ì´ë¯¸ì§€ ë³€í™˜
            img_base64 = pdf_page_to_base64(pdf_path, page_num)

            # Vision ë¶„ì„
            content = analyze_page_with_vision(img_base64, page_num)

            # í‚¤ì›Œë“œ ì¶”ì¶œ (ìš”ì•½ì—ì„œ)
            keywords = extract_keywords_from_content(content)

            page_data = {
                "page": page_num + 1,
                "content": content,
                "keywords": keywords
            }
            pages_data.append(page_data)

            print("âœ…")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜: {e}")
            pages_data.append({
                "page": page_num + 1,
                "content": f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                "keywords": []
            })

    # ê²°ê³¼ ë°ì´í„° êµ¬ì„±
    result = {
        "company": company_name,
        "year": year,
        "version": version,
        "filename": filename,
        "total_pages": total_pages,
        "pages": pages_data
    }

    return result


def extract_keywords_from_content(content):
    """ì»¨í…ì¸ ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
    # í‚¤ì›Œë“œ ì„¹ì…˜ ì°¾ê¸°
    keywords = []
    if "í‚¤ì›Œë“œ" in content:
        # í‚¤ì›Œë“œ ë¼ì¸ ì°¾ê¸°
        lines = content.split('\n')
        for line in lines:
            if "í‚¤ì›Œë“œ" in line and ":" in line:
                # í‚¤ì›Œë“œ ì¶”ì¶œ
                keyword_part = line.split(":")[-1]
                # ì‰¼í‘œë‚˜ ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
                keywords = [k.strip() for k in re.split(r'[,ØŒã€]', keyword_part) if k.strip()]
                break
    return keywords[:5]  # ìµœëŒ€ 5ê°œ


def save_to_json(result, company_name):
    """ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    company_dir = EXTRACTED_DIR / company_name
    company_dir.mkdir(parents=True, exist_ok=True)

    # íŒŒì¼ëª… ìƒì„± (ë²„ì „ì´ ìˆìœ¼ë©´ ì¶”ê°€)
    year = result.get('year', 'unknown')
    version = result.get('version')
    if version:
        json_filename = f"{year}_{company_name}_ESG_{version}.json"
    else:
        json_filename = f"{year}_{company_name}_ESG.json"
    json_path = company_dir / json_filename

    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(result, f, ensure_ascii=False, indent=2)

    print(f"   ğŸ’¾ JSON ì €ì¥: {json_path}")
    return json_path


def save_to_chromadb(result, collection):
    """ê²°ê³¼ë¥¼ ChromaDBì— ì €ì¥"""
    company = result['company']
    year = result['year']
    version = result.get('version')

    documents = []
    metadatas = []
    ids = []

    for page_data in result['pages']:
        page_num = page_data['page']
        content = page_data['content']
        keywords = page_data.get('keywords', [])

        # ë¬¸ì„œ ID ìƒì„± (ë²„ì „ì´ ìˆìœ¼ë©´ ì¶”ê°€)
        if version:
            doc_id = f"{company}_{year}_{version}_page_{page_num}"
        else:
            doc_id = f"{company}_{year}_page_{page_num}"

        documents.append(content)
        metadatas.append({
            "company": company,
            "year": year,
            "version": version if version else "",
            "page": page_num,
            "filename": result['filename'],
            "keywords": ", ".join(keywords)
        })
        ids.append(doc_id)

    # ChromaDBì— ì¶”ê°€
    collection.add(
        documents=documents,
        metadatas=metadatas,
        ids=ids
    )

    print(f"   ğŸ—„ï¸  ChromaDB ì €ì¥: {len(documents)}ê°œ ë¬¸ì„œ")


def process_all_companies():
    """ëª¨ë“  íšŒì‚¬ì˜ ëª¨ë“  PDF ì²˜ë¦¬"""
    print("=" * 60)
    print("ğŸš€ ì „ì²´ PDF Vision ì²˜ë¦¬ ì‹œì‘")
    print("=" * 60)

    # ChromaDB ì´ˆê¸°í™”
    print("\nğŸ“¦ ChromaDB ì´ˆê¸°í™” ì¤‘...")
    chroma_client = chromadb.PersistentClient(path=str(CHROMA_DIR))

    # ì„ë² ë”© í•¨ìˆ˜ ì„¤ì •
    embedding_function = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="jhgan/ko-sroberta-multitask"
    )

    # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ í›„ ìƒˆë¡œ ìƒì„±
    try:
        chroma_client.delete_collection(name="esg_vision_collection")
    except:
        pass

    collection = chroma_client.create_collection(
        name="esg_vision_collection",
        embedding_function=embedding_function
    )
    print("   âœ… ChromaDB ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ")

    # íšŒì‚¬ í´ë” ëª©ë¡
    company_folders = [f for f in COMPANY_DIR.iterdir() if f.is_dir()]

    total_pdfs = 0
    processed_pdfs = 0

    # ì „ì²´ PDF ìˆ˜ ê³„ì‚°
    for company_folder in company_folders:
        pdf_files = list(company_folder.glob("*.pdf"))
        total_pdfs += len(pdf_files)

    print(f"\nğŸ“Š ì´ {len(company_folders)}ê°œ íšŒì‚¬, {total_pdfs}ê°œ PDF íŒŒì¼")

    # ê° íšŒì‚¬ë³„ ì²˜ë¦¬
    for company_folder in company_folders:
        company_name = company_folder.name
        pdf_files = sorted(company_folder.glob("*.pdf"))

        print(f"\n{'='*60}")
        print(f"ğŸ¢ íšŒì‚¬: {company_name} ({len(pdf_files)}ê°œ PDF)")
        print(f"{'='*60}")

        for pdf_file in pdf_files:
            processed_pdfs += 1
            print(f"\n[{processed_pdfs}/{total_pdfs}]", end="")

            try:
                # PDF ì²˜ë¦¬
                result = process_single_pdf(str(pdf_file), company_name)

                # JSON ì €ì¥
                save_to_json(result, company_name)

                # ChromaDB ì €ì¥
                save_to_chromadb(result, collection)

            except Exception as e:
                print(f"\nâŒ PDF ì²˜ë¦¬ ì‹¤íŒ¨: {pdf_file}")
                print(f"   ì˜¤ë¥˜: {e}")

    # ì™„ë£Œ
    print("\n" + "=" * 60)
    print("âœ… ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ëœ PDF: {processed_pdfs}/{total_pdfs}")
    print(f"   JSON ì €ì¥ ìœ„ì¹˜: {EXTRACTED_DIR}")
    print(f"   ChromaDB ìœ„ì¹˜: {CHROMA_DIR}")
    print("=" * 60)

    # ì €ì¥ëœ ë¬¸ì„œ ìˆ˜ í™•ì¸
    doc_count = collection.count()
    print(f"\nğŸ“ˆ ChromaDB ì´ ë¬¸ì„œ ìˆ˜: {doc_count}")


if __name__ == "__main__":
    process_all_companies()
